{"cells":[{"cell_type":"markdown","metadata":{"tags":[],"id":"FYLBfRi16iVE"},"source":["# SI 618: Data Manipulation and Analysis\n","## 07 Beyond regex: Natural Language Processing\n","### Dr. Chris Teplovs, School of Information, University of Michigan\n","<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a> This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.</small>\n","    \n","### Please ensure you have this version:\n","Version 2023.02.13.1.CT\n"]},{"cell_type":"markdown","metadata":{"id":"RX5VdpH66iVH"},"source":["### Top-Level Learning Objective\n","* To familiarize ourselves with the basics of NLP and how to implement NLP techniques in Python\n","   \n","### Things we'll learn on the way:\n","1. What the spaCy package does and why it's useful\n","2. Text processing steps:\n","   1. normalization,\n","   2. tokenization,\n","   3. stop word removal,\n","   4. lemmatization,\n","   5. part-of-speech tagging\n","   6. named entity recognition\n","   7. sentiment analysis\n","\n","### How you know you've learned:\n","* Complete Homework 5"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Q0OSBFBN6iVI","executionInfo":{"status":"ok","timestamp":1697131434375,"user_tz":240,"elapsed":5,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"T8LCvVHI6iVK","executionInfo":{"status":"ok","timestamp":1697131454016,"user_tz":240,"elapsed":19076,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}}},"outputs":[],"source":["import spacy"]},{"cell_type":"markdown","metadata":{"id":"VL4vhJSA6iVK"},"source":["# spaCy"]},{"cell_type":"markdown","metadata":{"id":"rlrurk4A6iVK"},"source":["- Fast, and extensible NLP package for Python\n","- <https://spacy.io/>\n","- NOTE: You will need to install this, and then (one time only as well) download the English corpus."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KALyV9Ak6iVL","executionInfo":{"status":"ok","timestamp":1697131479927,"user_tz":240,"elapsed":17916,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}},"outputId":"e5e71c1e-4053-4a15-b11f-4b33776d6884"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-10-12 17:24:25.354001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Collecting en-core-web-md==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.6.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n"]}],"source":["# Comment out the next line when you've run this cell successfully\n","# !python -m spacy download en_core_web_md"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KehdQndj6iVL","executionInfo":{"status":"ok","timestamp":1697131497625,"user_tz":240,"elapsed":4513,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}}},"outputs":[],"source":["# loading up the language model: English\n","# note that Windows users might need to figure out where\n","# the previous cell installed the library and change the following line accordingly\n","nlp = spacy.load('en_core_web_md')"]},{"cell_type":"markdown","metadata":{"id":"7DBIBhAV6iVM"},"source":["# 0. Data cleaning"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"NN3SdRPY6iVM","executionInfo":{"status":"ok","timestamp":1697131507942,"user_tz":240,"elapsed":204,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}}},"outputs":[],"source":["# from Make It Stick: The Science of Successful Learning\n","sentences = \"\"\"\n","Michael Young is a high-achieving fourth-year medical student at\n","Georgia Regents University who pulled himself up from rock bottom\n","by changing the way he studies.\n","Young entered medical school without the usual foundation of premed\n","coursework. His classmates all had backgrounds in biochemistry,\n","pharmacology, and the like. Medical school is plenty tough under\n","any circumstances, but in Young's case even more so for lack of a footing.\n","\"\"\""]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"f4SdGf8-6iVN","executionInfo":{"status":"ok","timestamp":1697131509472,"user_tz":240,"elapsed":183,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}},"outputId":"ef4f151e-4b2f-4660-b0f4-ee005be75402"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nMichael Young is a high-achieving fourth-year medical student at\\nGeorgia Regents University who pulled himself up from rock bottom\\nby changing the way he studies.\\nYoung entered medical school without the usual foundation of premed\\ncoursework. His classmates all had backgrounds in biochemistry,\\npharmacology, and the like. Medical school is plenty tough under\\nany circumstances, but in Young's case even more so for lack of a footing.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["sentences"]},{"cell_type":"markdown","metadata":{"id":"-Ugg9Zxy6iVN"},"source":["### Section goal: calculate the frequency of each word\n","- See which words are more frequent.\n","- Generate more meaningful summary for the above paragraph."]},{"cell_type":"markdown","metadata":{"id":"QGKYknP26iVN"},"source":["## 0-1. lowering the case"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IRoQM2be6iVN","executionInfo":{"status":"ok","timestamp":1697131523815,"user_tz":240,"elapsed":262,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}},"outputId":"92156359-ba4f-43f7-a15c-ffabd13b64df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":10}],"source":["type(sentences)"]},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"hXBfoW-l6iVO","executionInfo":{"status":"ok","timestamp":1697131526182,"user_tz":240,"elapsed":263,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}},"outputId":"a0bfeb16-28bb-476f-8924-9569ba5d7030"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nMichael Young is a high-achieving fourth-year medical student at\\nGeorgia Regents University who pulled himself up from rock bottom\\nby changing the way he studies.\\nYoung entered medical school without the usual foundation of premed\\ncoursework. His classmates all had backgrounds in biochemistry,\\npharmacology, and the like. Medical school is plenty tough under\\nany circumstances, but in Young's case even more so for lack of a footing.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}],"source":["sentences"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"ASjCkEI_6iVO","executionInfo":{"status":"ok","timestamp":1697131528474,"user_tz":240,"elapsed":175,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}}},"outputs":[],"source":["sent_low = sentences.lower()"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"kMyYZPRG6iVO","executionInfo":{"status":"ok","timestamp":1697131530889,"user_tz":240,"elapsed":185,"user":{"displayName":"Alexander Kutsupis","userId":"09648249560451949651"}},"outputId":"a400665f-293f-4679-c3a6-b04822a91bf2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nmichael young is a high-achieving fourth-year medical student at\\ngeorgia regents university who pulled himself up from rock bottom\\nby changing the way he studies.\\nyoung entered medical school without the usual foundation of premed\\ncoursework. his classmates all had backgrounds in biochemistry,\\npharmacology, and the like. medical school is plenty tough under\\nany circumstances, but in young's case even more so for lack of a footing.\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["sent_low"]},{"cell_type":"markdown","metadata":{"toc-hr-collapsed":false,"id":"U91E73f-6iVP"},"source":["## 0-2. remove punctuation and special characters"]},{"cell_type":"markdown","metadata":{"id":"ZWVpqwo-6iVP"},"source":["#### Exclude special characters one by one"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PznQR67R6iVP"},"outputs":[],"source":["# from https://www.programiz.com/python-programming/examples/remove-punctuation\n","punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~‘’''' # list of special characters you want to exclude\n","sent_low_pnct = \"\"\n","for char in sent_low:\n","    if char not in punctuations:\n","        sent_low_pnct = sent_low_pnct + char\n","\n","sent_low_pnct"]},{"cell_type":"markdown","metadata":{"id":"1OIj26A46iVQ"},"source":["#### Alternatively, we can use regular expression to remove punctuations\n","- So we don't have to list up all possible special characters that we want to remove\n","- https://docs.python.org/3.4/library/re.html\n","- https://en.wikipedia.org/wiki/Regular_expression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdAAB3zi6iVQ"},"outputs":[],"source":["sent_low"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDCvZOJn6iVQ"},"outputs":[],"source":["import re\n","sent_low_pnct2 = re.sub(r'[^\\w\\s]+', ' ', sent_low)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXh-L_a_6iVQ"},"outputs":[],"source":["sent_low_pnct2"]},{"cell_type":"markdown","metadata":{"id":"DmC-GuAP6iVQ"},"source":["- However, special character ```\\n``` (linebreak) still exists in both cases. Let's remove these additionally."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq3HeOHR6iVR"},"outputs":[],"source":["import os\n","os.linesep"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"QvYpsC5r6iVR"},"outputs":[],"source":["sent_low_pnct = sent_low_pnct.replace(os.linesep, \" \")\n","sent_low_pnct"]},{"cell_type":"markdown","metadata":{"id":"mfNGzbek6iVR"},"source":["### And one more way..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvmnilXY6iVR"},"outputs":[],"source":["import string\n","\n","string.punctuation"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"6dlSyQxY6iVS"},"outputs":[],"source":["table = str.maketrans(dict.fromkeys(string.punctuation))\n","no_punctuation= sent_low.translate(table)\n","\n","print(no_punctuation)"]},{"cell_type":"markdown","metadata":{"id":"nWvu1NZv6iVS"},"source":["Regular expressions:\n","\n","^ means \"beginning of string\"\n","UNLESS it's in [ ], in which case it means \"not\"\n","\n","r'^The' # means The at the beginning of a string\n","r'^[The]' # means any one of T or h or e at the beginning of a string\n","t'^[^The]' # means any character other than T,h,or e at the beginning"]},{"cell_type":"markdown","metadata":{"id":"nVmveuK-6iVS"},"source":["### So... at least 3 possible ways to replace characters!"]},{"cell_type":"markdown","metadata":{"id":"WcsuCHBK6iVS"},"source":["## 0-3. Remove stop words"]},{"cell_type":"markdown","metadata":{"id":"LTVnzQ3Q6iVS"},"source":["- Stop words usually refers to the most common words in a language\n","    - No single universal stopwords\n","    - Often stopwords are removed to improve the performance of NLP models\n","    - https://en.wikipedia.org/wiki/Stop_words\n","    - https://en.wikipedia.org/wiki/Most_common_words_in_English"]},{"cell_type":"markdown","metadata":{"id":"vFcJuQbZ6iVT"},"source":["#### Import the list of stop words from ```spaCy```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nu2nZmY6iVT"},"outputs":[],"source":["from spacy.lang.en.stop_words import STOP_WORDS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWKSvU8Y6iVT"},"outputs":[],"source":["STOP_WORDS"]},{"cell_type":"markdown","metadata":{"id":"7T6YNjLP6iVT"},"source":["#### Goal: We are going to count the frequency of each word from the paragraph, to see which words can be used to represent the paragraph's content."]},{"cell_type":"markdown","metadata":{"id":"o3tD3bJx6iVU"},"source":["#### What if we do not remove stopwords?"]},{"cell_type":"markdown","metadata":{"id":"ZroLWNxW6iVb"},"source":["- Note that our paragraph is stored as a single string object..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_o9fo5OE6iVb"},"outputs":[],"source":["sent_low_pnct"]},{"cell_type":"markdown","metadata":{"id":"bObkomVI6iVb"},"source":["- Split the paragraph into a list of words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qI_IPczu6iVb"},"outputs":[],"source":["words = sent_low_pnct.split()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kA2UxFM36iVc"},"outputs":[],"source":["words[:10]"]},{"cell_type":"markdown","metadata":{"id":"6Y7VvZRZ6iVc"},"source":["- Count the words from the list\n","- Words that can occur in any kind of paragraphs...?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5RVjvW8g6iVc"},"outputs":[],"source":["d = {}\n","for word in words:\n","    if word in d:\n","        d[word] = d[word] + 1\n","    else:\n","        d[word] = 1\n","d"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewdMUJW36iVc"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4X5fGIbg6iVd"},"outputs":[],"source":["Counter(words).most_common(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmQtQAQU6iVd"},"outputs":[],"source":["plt.figure(figsize=(45,10))\n","sns.countplot(x=words, order=pd.Series(words).value_counts().index)\n","# sns.countplot(words_nostop, order=[counted[0] for counted in Counter(words_nostop).most_common()])\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"zrixcWnw6iVd"},"source":["#### When we removed stopwords:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39eYPeoJ6iVd"},"outputs":[],"source":["words_nostop = list()\n","for word in words:\n","    if word not in STOP_WORDS:\n","        words_nostop.append(word)"]},{"cell_type":"markdown","metadata":{"id":"4jAMSyKJ6iVd"},"source":["### <font color=\"magenta\"> Q1: Re-implement the code in the previous cell using a list comprehension</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2SPQKqY6iVe"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"dpvFZ8fx6iVe"},"source":["- More comprehensible, and unique list or words!"]},{"cell_type":"markdown","metadata":{"id":"hi2lQSbN6iVe"},"source":["### <font color=\"magenta\">Q2: Use a `Counter` to find the frequencies of each word in the `words_nostop` list.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_jeK2Mq6iVe"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"OrN1xAYj6iVe"},"source":["### <font color=\"magenta\">Q3: Create a bar chart showing the frequencies of the 10 most common words, alphabetically sorted.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsD3EFEb6iVf"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"IcJqyEYD6iVf"},"source":["# 1. Extracting linguistic features from spaCy"]},{"cell_type":"markdown","metadata":{"id":"kf6w80SG6iVf"},"source":["## 1-1. Tokenize\n","- Token: a semantic unit for analysis\n","    - (Loosely) equal term for word\n","        - ```sent_low_pnct.split()```\n","    - Tricky cases\n","        - aren't $\\rightarrow$ ![](https://nlp.stanford.edu/IR-book/html/htmledition/img88.png) ![](https://nlp.stanford.edu/IR-book/html/htmledition/img89.png) ? ![](https://nlp.stanford.edu/IR-book/html/htmledition/img86.png) ?\n","        - O'Neil $\\rightarrow$ ![](https://nlp.stanford.edu/IR-book/html/htmledition/img83.png) ? ![](https://nlp.stanford.edu/IR-book/html/htmledition/img84.png) ![](https://nlp.stanford.edu/IR-book/html/htmledition/img81.png) ?\n","        - https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html\n","- In ```spaCy```:\n","    - Many token types, like word, punctuation symbol, whitespace, etc."]},{"cell_type":"markdown","metadata":{"id":"ymNAm-5k6iVf"},"source":["### Let's dissect the sentence!\n","\n","- initiating the ```spaCy``` object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y9bZgt2-6iVf"},"outputs":[],"source":["# examples partially taken from https://nlpforhackers.io/complete-guide-to-spacy/\n","import spacy\n","nlp = spacy.load('en_core_web_md')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1tx27Nu6iVg"},"outputs":[],"source":["type(nlp)"]},{"cell_type":"markdown","metadata":{"id":"QQhJBMaH6iVg"},"source":["- Our sentence: \"Hello World!\"\n","    - Pass the sentence string to the ```spaCy``` object ```nlp```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pk21-LWu6iVg"},"outputs":[],"source":["doc = nlp(\"Hello World!\")"]},{"cell_type":"markdown","metadata":{"id":"lHzFCnJe6iVg"},"source":["- The sentence is considered as a short document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K74zdyUl6iVg"},"outputs":[],"source":["print(type(doc), doc)"]},{"cell_type":"markdown","metadata":{"id":"bdzcD_Ut6iVh"},"source":["- As importing the sentence string above, ```spaCy``` split the sentence into tokens (tokenization!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ryUVJ5WX6iVh"},"outputs":[],"source":["for i,token in enumerate(doc):\n","    print(i, token)"]},{"cell_type":"markdown","metadata":{"id":"Hj3lyUYR6iVh"},"source":["- With index information (location from the sentence) of each token"]},{"cell_type":"markdown","metadata":{"id":"dLIcJbja6iVh"},"source":["| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10| 11|\n","|---|---|---|---|---|---|---|---|---|---|---|---|\n","| H | e | l | l | o | _ | W | o | r | l | d | ! |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vX5fPnHU6iVh"},"outputs":[],"source":["for i, token in enumerate(doc):\n","    print(i, token.text, token.idx)\n"]},{"cell_type":"markdown","metadata":{"id":"L5K2miYO6iVi"},"source":["- And many more!\n","    - https://spacy.io/api/token#attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EpXhbYaG6iVi"},"outputs":[],"source":["sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdVmHwaj6iVi"},"outputs":[],"source":["doc = nlp(sentences)\n","\n","print(\"text\\tidx\\tlemma\\tlower\\tpunct\\tspace\\tshape\\tPOS\")\n","for token in doc:\n","    if token.is_space:\n","        print(\"SPACE\")\n","    else:\n","        print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n","            token.text,\n","            token.idx,\n","            token.lemma_,\n","            token.lower_,\n","            token.is_punct,\n","            token.is_space,\n","            token.shape_,\n","            token.pos_\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"6GRxzewY6iVi"},"outputs":[],"source":["doc = nlp(sentences)\n","\n","print(\"text\\tidx\\tlemma\\tlower\\tpunct\\tspace\\tshape\\tPOS\")\n","for token in doc:\n","    if token.is_space:\n","        print(\"SPACE\")\n","    else:\n","        print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n","            token.text,\n","            token.idx,\n","            token.lemma_,\n","            token.lower_,\n","            token.is_punct,\n","            token.is_space,\n","            token.shape_,\n","            token.pos_\n","    ))\n"]},{"cell_type":"markdown","metadata":{"id":"qwDh_kQ56iVi"},"source":["## 1-2. Sentence detection"]},{"cell_type":"markdown","metadata":{"id":"lPXjP4V26iVj"},"source":["- For the document with multiple sentences, we would need to separate  each sentence.\n","- In ```spaCy```, the job is more convenient (and would cause less mistakes) than using regular expression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neqe1-ut6iVj"},"outputs":[],"source":["sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-D-3I9uk6iVj"},"outputs":[],"source":["# same document, but initiate as the spaCy object...\n","doc = nlp(sentences)"]},{"cell_type":"markdown","metadata":{"id":"gtoL6i7I6iVj"},"source":["- Sentences are stored as a generator object\n","    - Instead of storing sentences as a list, each sentence is stored as a item in the generator object\n","    - Iteratable (i.e., can be used in a for loop)\n","    - More efficient memory use\n","    - https://wiki.python.org/moin/Generators"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FOuhHiWe6iVk"},"outputs":[],"source":["doc.sents"]},{"cell_type":"markdown","metadata":{"id":"hRzgbNwE6iVk"},"source":["- Printing sentences with the index number"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"VeyF7whl6iVk"},"outputs":[],"source":["for i, sent in enumerate(doc.sents):\n","    print(i, sent)"]},{"cell_type":"markdown","metadata":{"id":"ExGPOLPD6iVk"},"source":["## 1-3. POS tagging"]},{"cell_type":"markdown","metadata":{"id":"OCVVKZxy6iVl"},"source":["- I want to find words with particular part-of-speech!\n","- Different part-of-speech words carry different information\n","    - e.g., noun (subject), verb (action term), adjective (quality of the object)\n","- https://spacy.io/usage/linguistic-features#pos-tagging"]},{"cell_type":"markdown","metadata":{"id":"RPOUd9Ne6iVl"},"source":["- Yelp review!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XLO7AEzp6iVl"},"outputs":[],"source":["# from https://www.yelp.com/biz/ajishin-novi?hrid=juA4Zn2TX7845vNFn4syBQ&utm_campaign=www_review_share_popup&utm_medium=copy_link&utm_source=(direct)\n","doc = nlp(\"\"\"One of the best Japanese restaurants in Novi. Simple food, great taste, amazingly price. I visit this place a least twice month.\"\"\")"]},{"cell_type":"markdown","metadata":{"id":"2X-Std8Z6iVl"},"source":["- multiple sentences exist in a document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGKOOb6Y6iVl"},"outputs":[],"source":["for i, sent in enumerate(doc.sents):\n","    print(i, sent)"]},{"cell_type":"markdown","metadata":{"id":"VitcwSJp6iVl"},"source":["- Question: which words are adjective (ADJ)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mk-Bn8iN6iVm"},"outputs":[],"source":["for i, sent in enumerate(doc.sents):\n","    #print(\"__sentence__:\", i)\n","    #print(\"_token_ \\t _POS_\")\n","    for token in sent:\n","        if token.pos_ == 'ADJ':\n","            print(token.text, \"\\t\", token.pos_)"]},{"cell_type":"markdown","metadata":{"id":"E6CbAoFY6iVm"},"source":["## Named Entity Recognition"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"mT44uQUX6iVm"},"outputs":[],"source":["doc = nlp(sentences)\n","print([(X.text, X.label_) for X in doc.ents])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EjLBCCe6iVm"},"outputs":[],"source":["url = 'https://fivethirtyeight.com/features/remembering-alex-trebek-the-man-with-all-the-answers/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUthM6Y16iVm"},"outputs":[],"source":["!pip install html5lib bs4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DhGS5Kg6iVm"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","import requests\n","import re\n","def url_to_string(url):\n","    res = requests.get(url)\n","    html = res.text\n","    soup = BeautifulSoup(html, 'html5lib')\n","    for script in soup([\"script\", \"style\", 'aside']):\n","        script.extract()\n","    return \" \".join(re.split(r'[\\n\\t]+', soup.get_text()))\n","ny_bb = url_to_string(url)\n","article = nlp(ny_bb)\n","len(article.ents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yvlRd4F6iVn"},"outputs":[],"source":["article"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IfvhMWZS6iVn"},"outputs":[],"source":["labels = [(x.label_,x.text) for x in article.ents]\n","Counter(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ujjc0ZE_6iVn"},"outputs":[],"source":["labels = [x.label_ for x in article.ents]\n","Counter(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44n_UUff6iVn"},"outputs":[],"source":["labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWkC4BPU6iVn"},"outputs":[],"source":["plt.figure(figsize=(45,10))\n","sns.countplot(x=labels, order=pd.Series(labels).value_counts().index)\n","# sns.countplot(words_nostop, order=[counted[0] for counted in Counter(words_nostop).most_common()])\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P9ZuEgtU6iVo"},"source":["# BREAK"]},{"cell_type":"markdown","metadata":{"id":"HNy7EmdK6iVo"},"source":["# NLP Part II"]},{"cell_type":"markdown","metadata":{"id":"o6eGvwwh6iVo"},"source":["# 1. Word embedding"]},{"cell_type":"markdown","metadata":{"id":"l-b6w5p96iVp"},"source":["#### Word2Vec\n","- Developed by [Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n","- Represent the meaning of the words as a vector\n","    - Vector: numeric array\n","    - Output of a neural network model that predicts the next word\n","- Surprisingly, many different semantic informations can be represented from word vectors of ```Word2Vec```\n","- (More explanation in here: https://www.tensorflow.org/tutorials/representation/word2vec)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60PEPtOV6iVp"},"outputs":[],"source":["! pip install gensim"]},{"cell_type":"markdown","metadata":{"id":"9dBMhU7B6iVp"},"source":["You will also need to download a pretrained language model: https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"clRDiC_i6iVp"},"outputs":[],"source":["import gensim"]},{"cell_type":"markdown","metadata":{"id":"30fdkfwX6iVp"},"source":["Change the filepath in the next cell to correspond to the location of the pretrained model file you downloaded above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vz-rrFye6iVq"},"outputs":[],"source":["w2v_mod = gensim.models.KeyedVectors.load_word2vec_format(\"~/Downloads/GoogleNews-vectors-negative300-SLIM.bin.gz\", binary=True)"]},{"cell_type":"markdown","metadata":{"id":"pJRwP8vE6iVq"},"source":["## 1-1. Calculating similarity between words"]},{"cell_type":"markdown","metadata":{"id":"ObBQL_ur6iVq"},"source":["- Q: What's similarity between *school* and *student*?"]},{"cell_type":"markdown","metadata":{"id":"Mkh6zTYe6iVq"},"source":["- the word vector for *school* looks like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS4G6sGA6iVq"},"outputs":[],"source":["w2v_mod['school']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2T4_ddkw6iVq"},"outputs":[],"source":["len(w2v_mod['school'])"]},{"cell_type":"markdown","metadata":{"id":"zBaJtmD-6iVq"},"source":["- and the word vector for *student* looks like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFSv5JzY6iVr"},"outputs":[],"source":["w2v_mod['student']"]},{"cell_type":"markdown","metadata":{"id":"pJYJAonh6iVr"},"source":["- the similarity between two word vectors is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PO08HVbc6iVr"},"outputs":[],"source":["w2v_mod.similarity('school', 'student')"]},{"cell_type":"markdown","metadata":{"id":"wMDJnctI6iVr"},"source":["### <font color='magenta'> Q4: Find a word that is more similar to school using this model </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bDAJfARL6iVr"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"q1cW69X_6iVr"},"source":["### <font color='magenta'>Q5 Find two words that have a cosine similarity less than .1 </font>\n","- How would you interprete the results?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96chN3bG6iVr"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"QYH2_t9D6iVs"},"source":["### <font color='magenta'> Q6 Try some other words. Any other interesting findings? </font>\n","- Give 2 more examples.\n","- How would you interprete the results?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pvQIGjn6iVs"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"DE--GoYo6iVs"},"source":["### Let's try with some example: words in a semantic space\n","$\\rightarrow$ https://projector.tensorflow.org"]},{"cell_type":"markdown","metadata":{"id":"BvuyOEfx6iVt"},"source":["### <font color='magenta'> Q7 Any interesting findings from TensorFlow Projector page? </font>"]},{"cell_type":"markdown","metadata":{"id":"EluTfEcI6iVt"},"source":["(type in your response here)"]},{"cell_type":"markdown","metadata":{"id":"MBddTr536iVt"},"source":["## 1-2. Analogy from word vectors"]},{"cell_type":"markdown","metadata":{"id":"OUdKB8QW6iVt"},"source":["<img src=\"https://www.tensorflow.org/images/linear-relationships.png\" width=\"800\">"]},{"cell_type":"markdown","metadata":{"id":"MewP9SEk6iVt"},"source":["#### Can we approximate the relationship between words by doing - and + operations?"]},{"cell_type":"markdown","metadata":{"id":"xImNtzTj6iVt"},"source":["- $woman - man + king \\approx ?$\n","- How this works?\n","    - $woman:man \\approx x:king $\n","    - $\\rightarrow woman - man \\approx x - king $\n","    - $\\rightarrow woman - man + king \\approx x$\n","    - List top-10 words ($x$) that can solve the equation!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tew2bVMU6iVt"},"outputs":[],"source":["w2v_mod.most_similar(positive=['woman', 'king'], negative=['man'])"]},{"cell_type":"markdown","metadata":{"id":"9ARoEWPy6iVt"},"source":["- $Spain - Germany + Berlin \\approx ?$\n","    - $\\rightarrow Spain - Germany \\approx x -  Berlin $"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnI85Quk6iVu"},"outputs":[],"source":["w2v_mod.most_similar(positive=['Spain', 'Berlin'], negative=['Germany'])"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"6-XXkL356iVu"},"source":["### <font color='magenta'> Q8 Any other interesting examples? </font>\n","- Give 3 more examples.\n","- How would you interprete the results?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pjN0Ijtx6iVu"},"outputs":[],"source":["# insert your code here"]},{"cell_type":"markdown","metadata":{"id":"cqc28f9_6iVu"},"source":["## 1-3. Constructing interpretable semantic scales"]},{"cell_type":"markdown","metadata":{"id":"L0Yl3QU96iVu"},"source":["- So far, we saw that word vectors effectively carries (although not perfect) the semantic information.\n","- Can we design something more interpretable results from using the semantic space?"]},{"cell_type":"markdown","metadata":{"id":"TDdl_g326iVu"},"source":["- Let's re-try with real datapoints in [here](https://projector.tensorflow.org): *politics* words in a *bad-good* PCA space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMw9qj2j6iVv"},"outputs":[],"source":["from scipy import spatial\n","\n","def cosine_similarity(x, y):\n","    return(1 - spatial.distance.cosine(x, y))"]},{"cell_type":"markdown","metadata":{"id":"HoF3Pkjv6iVv"},"source":["- Can we regenerate this results with our embedding model?"]},{"cell_type":"markdown","metadata":{"id":"BarqBLh06iVv"},"source":["### Let's plot words in the 2D space\n","- Using Bad & Good axes\n","- Calculate cosine similarity between an evaluating word (violence, discussion, and issues) with each scale's end (bad, and good)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylUkDguo6iVv"},"outputs":[],"source":["pol_words_sim_2d = pd.DataFrame([[cosine_similarity(w2v_mod['violence'], w2v_mod['good']), cosine_similarity(w2v_mod['violence'], w2v_mod['bad'])],\n","                                 [cosine_similarity(w2v_mod['discussion'], w2v_mod['good']), cosine_similarity(w2v_mod['discussion'], w2v_mod['bad'])],\n","                                 [cosine_similarity(w2v_mod['issues'], w2v_mod['good']), cosine_similarity(w2v_mod['issues'], w2v_mod['bad'])]],\n","                                index=['violence', 'discussion', 'issues'], columns=['good', 'bad'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0yzjwJ96iVv"},"outputs":[],"source":["pol_words_sim_2d"]},{"cell_type":"markdown","metadata":{"id":"yocyTp5F6iVv"},"source":["- If we plot this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmJTspch6iVw"},"outputs":[],"source":["sns.scatterplot(x='good', y='bad', data=pol_words_sim_2d, hue=pol_words_sim_2d.index)"]},{"cell_type":"markdown","metadata":{"id":"6-yKk8fw6iVw"},"source":["- violence: less good, more bad\n","- discussion: less bad, more good\n","- issues: both bad and good"]},{"cell_type":"markdown","metadata":{"id":"CqfxQZkL6iVw"},"source":["### Can we do this in an 1D scale?\n","(bad) --------------------?---- (good)"]},{"cell_type":"markdown","metadata":{"id":"G_wIjDRH6iVw"},"source":["- First, let's create the vector for *bad-good* scale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nldwys3K6iVx"},"outputs":[],"source":["scale_bad_good = w2v_mod['good'] - w2v_mod['bad']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3urpRle6iVx"},"outputs":[],"source":["cosine_similarity(w2v_mod['good'], w2v_mod['bad'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyZ0UjyK6iVx"},"outputs":[],"source":["len(scale_bad_good)"]},{"cell_type":"markdown","metadata":{"id":"k2UOpAlb6iVx"},"source":["- Calculate the cosine similarity score of the word *violence* in the *bad-good* scale\n","    - $sim(V(violence), V(bad) - V(good))$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAjE_IZT6iVy"},"outputs":[],"source":["violence_score = cosine_similarity(w2v_mod['violence'], scale_bad_good)\n","violence_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guHnGNWb6iVy"},"outputs":[],"source":["discussion_score = cosine_similarity(w2v_mod['discussion'], scale_bad_good)\n","discussion_score"]},{"cell_type":"markdown","metadata":{"id":"hx8gmePB6iVy"},"source":["# 2. Sentiment Analysis with NLTK"]},{"cell_type":"markdown","metadata":{"id":"V5xW5nLF6iVy"},"source":["\"The Natural Language Toolkit (NLTK) is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language.\"\n","for more information see: https://www.nltk.org/"]},{"cell_type":"markdown","metadata":{"id":"UIl9OoZs6iVy"},"source":["We are going to use NLTK and Spacy to determine if text expresses positive sentiment, negative sentiment, or if it's neutral."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3POS5j56iVz"},"outputs":[],"source":["# adapted from https://github.com/Jcharis/Natural-Language-Processing-Tutorials/blob/master/NLP%20with%20SpaCy-%20Adding%20Extensions%20Attributes%20in%20SpaCy(How%20to%20use%20sentiment%20analysis%20in%20SpaCy).ipynb\n","import nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRdgzLrL6iVz"},"outputs":[],"source":["!python -m nltk.downloader book"]},{"cell_type":"markdown","metadata":{"id":"GLt9vkJj6iVz"},"source":["\"VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\"\n","\n","for more see: https://github.com/cjhutto/vaderSentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSwE8sco6iV0"},"outputs":[],"source":["nltk.download('vader_lexicon')"]},{"cell_type":"markdown","metadata":{"id":"kGGcYx4h6iV0"},"source":["We are going to extend the spacy functionality with the SentimentIntensityAnalyzer function from NLTK."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wTmB1nCH6iV0"},"outputs":[],"source":["from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","sent_analyzer = SentimentIntensityAnalyzer()\n","def sentiment_scores(docx):\n","    return sent_analyzer.polarity_scores(docx.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDIM4L_p6iV0"},"outputs":[],"source":["import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAYm8dhK6iV1"},"outputs":[],"source":["# loading up the language model: English\n","nlp = spacy.load('en_core_web_md')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlZQYjtv6iV1"},"outputs":[],"source":["from spacy.tokens import Doc\n","Doc.set_extension(\"sentimenter\",getter=sentiment_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzJNxAPv6iV1"},"outputs":[],"source":["nlp(\"This introduction was great but the conclusions were terrible\")._.sentimenter"]},{"cell_type":"markdown","metadata":{"id":"q60msdU96iV2"},"source":["Let's apply this sentiment analysis to product reviews on Amazon"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YSZFVqPD6iV2"},"outputs":[],"source":["r = pd.read_csv('https://raw.githubusercontent.com/umsi-data-science/data/main/small_reviews.csv',index_col=0)\n","#random sample of original dataset at https://www.kaggle.com/snap/amazon-fine-food-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASXQJrkG6iV2"},"outputs":[],"source":["r.head()"]},{"cell_type":"markdown","metadata":{"id":"1_yAHx856iV2"},"source":["We'll use the apply function to transform text with spacy's nlp function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQjXp1Nm6iV2"},"outputs":[],"source":["r['rating'] = r['Text'].apply(lambda x: nlp(x)._.sentimenter['compound'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQLA8a1Y6iV3"},"outputs":[],"source":["r[['Score','rating','Text']].head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klUe-CqB6iV3"},"outputs":[],"source":["r.iloc[6].Text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpGWm7Lk6iV3"},"outputs":[],"source":["sns.scatterplot(x='Score',y='rating',data=r)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GvSPI5v6iV3"},"outputs":[],"source":["import statsmodels.api as sm\n","import statsmodels.formula.api as smf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VzlJ63A6iV4"},"outputs":[],"source":["model0 = smf.ols(\"rating ~ Score \", data=r)\n","model0.fit().summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htbWtGwE6iV4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bteETzQr6iV4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"toc-autonumbering":false,"toc-showcode":false,"toc-showmarkdowntxt":false,"toc-showtags":false,"vscode":{"interpreter":{"hash":"90fd63f6819c340b438e5c293ddfb31e39be622c688d56bb62c76ae0c0566a54"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}